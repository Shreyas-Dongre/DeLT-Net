{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Authors: Shreyas,Shrushti**\n",
        "\n",
        "Model Trained on Nvidia A-100 80 Gb Gpu."
      ],
      "metadata": {
        "id": "yXrSUR2yjBiu"
      },
      "id": "yXrSUR2yjBiu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024ef16a",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "024ef16a",
        "papermill": {
          "duration": 0.03447,
          "end_time": "2022-01-04T16:42:51.061663",
          "exception": false,
          "start_time": "2022-01-04T16:42:51.027193",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#breakdown.py\n",
        "import os\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "'''\n",
        "def chunks(l, n):\n",
        "    n = max(1, n)\n",
        "    return (l[i:i+n] for i in range(0, len(l), n))\n",
        "'''\n",
        "sponsor_times = pd.read_csv('/content/drive/MyDrive/PROJECT/0part.csv', sep=',')   #--> spons sub dataset\n",
        "sponsor_times.head()\n",
        "\n",
        "number_of_entries = len(sponsor_times['category'])\n",
        "\n",
        "for category in set(sponsor_times['category']):\n",
        "    count = np.count_nonzero(sponsor_times['category'] == category)\n",
        "    print(f'{category}: {count}/{number_of_entries} ({count / number_of_entries * 100})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5khemm1Lc064",
        "outputId": "84f936d0-0bff-4fd2-9b3f-02a3693fa4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-a8bcd83dd6ba>:14: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  sponsor_times = pd.read_csv('/content/drive/MyDrive/PROJECT/0part.csv', sep=',')   #--> spons sub dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filler: 33396/3699847 (0.9026319196442447)\n",
            "exclusive_access: 1134/3699847 (0.030649916064096706)\n",
            "sponsor: 1194642/3699847 (32.28895681361959)\n",
            "outro: 613052/3699847 (16.569658150728937)\n",
            "preview: 65203/3699847 (1.7623161173962059)\n",
            "poi_highlight: 90940/3699847 (2.4579394769567497)\n",
            "moreCategories: 4/3699847 (0.00010811257870933582)\n",
            "selfpromo: 322032/3699847 (8.703927486731208)\n",
            "intro: 886386/3699847 (23.957369047963333)\n",
            "music_offtopic: 152804/3699847 (4.130008619275338)\n",
            "interaction: 340254/3699847 (9.196434339041588)\n"
          ]
        }
      ],
      "id": "5khemm1Lc064"
    },
    {
      "cell_type": "code",
      "source": [
        "#keep vid id\n",
        "import pandas as pd\n",
        "\n",
        "# Filters the csv so that only the videoID column is preserved.\n",
        "sponsor_times = pd.read_csv('/content/drive/MyDrive/PROJECT/0part.csv', sep=',')  #--> spons sub dataset (same as before)\n",
        "video_ids = sponsor_times[['videoID']]\n",
        "video_ids = video_ids[sponsor_times['category'] == 'sponsor']\n",
        "video_ids.to_csv('/content/0part_new.csv') #temp csv will be saved in your LOCAL Runtime"
      ],
      "metadata": {
        "id": "Pad2AmTqc1CG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020e2b5e-dc55-4f4a-ec4e-afbcc82043e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-61072a453b3b>:5: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  sponsor_times = pd.read_csv('/content/drive/MyDrive/PROJECT/0part.csv', sep=',')  #--> spons sub dataset (same as before)\n"
          ]
        }
      ],
      "id": "Pad2AmTqc1CG"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "class DBSegment:\n",
        "\tcategory: str\n",
        "\tstartTime: float\n",
        "\tendTime: float\n",
        "\tUUID: str\n",
        "\tuserID: str\n",
        "\tvotes: int\n",
        "\tviews: int\n",
        "\tlocked: int\n",
        "\thidden: int\n",
        "\tshadowHidden: int\n",
        "\tvideoID: str\n",
        "\tvideoDuration: int\n",
        "\treputation: int\n",
        "\thashedVideoID: str\n",
        "\ttimeSubmitted: int\n",
        "\tuserAgent: str\n",
        "\tservice: str\n",
        "\tdescription: str\n",
        "\n",
        "class OverlappingSegmentGroup:\n",
        "\tdef __init__(self):\n",
        "\t\tself.segments: List[DBSegment] = []\n",
        "\t\tself.votes = 0\n",
        "\n",
        "def build_segment_groups(segments: List[DBSegment]) -> List[OverlappingSegmentGroup]:\n",
        "\t\"\"\"\n",
        "\tThis function will find segments that are contained inside of eachother, called similar segments.  (Timestamps)\n",
        "\tSegments with less than -1 votes are already ignored before this function is called.\n",
        "\n",
        "\tBased on https://github.com/ajayyy/SponsorBlockServer/blob/e74b985304443b17b429c5c82696c7a03e78a166/src/routes/getSkipSegments.ts#L276\n",
        "\t\"\"\"\n",
        "\n",
        "\t# Create groups of segments that are similar to eachother\n",
        "\t# Segments must be sorted by their startTime so that we can build groups chronologically:\n",
        "\t# 1. As long as the segments' startTime fall inside the currentGroup, we keep adding them to that group\n",
        "\t# 2. If a segment starts after the end of the currentGroup (> cursor), no other segment will ever fall\n",
        "\t#    inside that group (because they're sorted) so we can create a new one\n",
        "\toverlappingSegmentsGroups: List[OverlappingSegmentGroup] = []\n",
        "\tcurrentGroup = None\n",
        "\tcursor = -1 # -1 to make sure that, even if the 1st segment starts at 0, a new group is created\n",
        "\tfor segment in segments:\n",
        "\t\tif segment.startTime >= cursor:\n",
        "\t\t\tcurrentGroup = OverlappingSegmentGroup()\n",
        "\t\t\toverlappingSegmentsGroups.append(currentGroup)\n",
        "\n",
        "\t\tcurrentGroup.segments.append(segment)\n",
        "\t\t# only if it is a positive vote, otherwise it is probably just a sponsor time with slightly wrong time\n",
        "\t\tif segment.votes > 0:\n",
        "\t\t\tcurrentGroup.votes += segment.votes\n",
        "\n",
        "\t\tcursor = max(cursor, segment.endTime)\n",
        "\n",
        "\treturn overlappingSegmentsGroups\n",
        "\n",
        "def get_best_segment(group: OverlappingSegmentGroup):\n",
        "\t\"\"\"\n",
        "\tSponsorBlock chooses a segment from an overlap group randomly by using the\n",
        "\tvotes property as a weight. This is done so that all segments can have a\n",
        "\tchance of appearing and makes sense in that system, but here we just\n",
        "\twant the best possible match, hence we pick the segment with the highest vote.\n",
        "\t\"\"\"\n",
        "\n",
        "\treturn max(group.segments, key=lambda segment: segment.votes)"
      ],
      "metadata": {
        "id": "G9-NUNBYtL9t"
      },
      "execution_count": null,
      "outputs": [],
      "id": "G9-NUNBYtL9t"
    },
    {
      "cell_type": "code",
      "source": [
        "#main.py TRYYYYYYYYYYYYYYYYYYYYYYy------------------222222222222222222\n",
        "from concurrent.futures import thread\n",
        "from glob import glob\n",
        "import os\n",
        "import sys\n",
        "import multiprocessing.dummy\n",
        "import subprocess\n",
        "from sys import stdout\n",
        "import threading\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "import requests, json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# File with column 'videoID'\n",
        "INPUT_FILENAME = '/content/0part_new.csv'                                           #'/content/testpartiii.csv'\n",
        "# File to use to save job state\n",
        "STATUS_FILENAME = '/content/drive/MyDrive/CNN_Dataset_Dump/shreyas/slot_1/statuscnn.csv' #------> change location as per your drive and slot folder!\n",
        "                                           # status csv to save job state. if loop fails and u hv to restart then you will be able to restart frm where u left!!\n",
        "# Number of concurrent yt-dlp scripts to run\n",
        "NUM_THREADS = int('32')\n",
        "\n",
        "iteration=0\n",
        "chunk_id=0\n",
        "processed_df=None\n",
        "\n",
        "# Runtime stats to display progress\n",
        "processed = 0\n",
        "gold = 0\n",
        "silver = 0\n",
        "start = time.time()\n",
        "# Synchronisation for the variables above and the STATUS_FILENAME\n",
        "lock = threading.Lock()\n",
        "sponsor_times = '/content/drive/MyDrive/CNN_Dataset_Dump/shreyas/0part.csv'  #------> spons sub - dataset (same as first cell)\n",
        "sponsorml_df = pd.read_csv(sponsor_times)                                          #FF\n",
        "grouped_df = sponsorml_df.groupby(by=[\"videoID\"])\n",
        "\n",
        "try:\n",
        "    # Try to restore the state from last time\n",
        "    results = pd.read_csv(STATUS_FILENAME, index_col=False)\n",
        "    print('Continuing...')\n",
        "except:\n",
        "    results = pd.DataFrame(columns=['videoID', 'status'])      #'Intensity_Score''TimeRangeStart''Marker_Duration'\n",
        "\n",
        "processed_last_session = len(results['videoID'])\n",
        "\n",
        "\n",
        "def vid_process(vid_id):\n",
        "\n",
        "        segments = grouped_df.get_group(vid_id)   #get sponsor segment time stamps fetched from the main public dataset.\n",
        "        if len(segments) > 0:\n",
        "          print('first if')\n",
        "          segments = [segment for _, segment in segments.iterrows()\n",
        "          if segment.category == 'sponsor' and segment.votes >= 1]\n",
        "          # Filter out similar timestamps\n",
        "          segments = [get_best_segment(group) for group in build_segment_groups(segments)]\n",
        "          segment_times = [(round(segment.startTime,0), round(segment.endTime,0)) for segment in segments]\n",
        "          print('got seg times',len(segment_times))\n",
        "          # ----------------------------LOGIC-------------------------------------------------------------------------------------------------------\n",
        "          if len(segment_times) > 0:\n",
        "            final_sponsor_segments=[]\n",
        "            for i in range(len(segment_times)):\n",
        "                print('got secfor')\n",
        "                temp=1\n",
        "                for j in range(0,len(segment_times[i]),2):\n",
        "                  start_spons = segment_times[i][j]\n",
        "                  start_spons = int(round(start_spons,0))\n",
        "                  print('start_spons is',start_spons)\n",
        "\n",
        "                for k in range(1,len(segment_times[i]),2):\n",
        "                  end_spons = segment_times[i][k]\n",
        "                  end_spons = int(round(end_spons,0))\n",
        "                  print('end_spons is',end_spons)\n",
        "\n",
        "                s = end_spons-start_spons\n",
        "                sby2 = int(round(s/2,0))\n",
        "                start_prespons = max(start_spons-sby2,0)\n",
        "                start_prespons = int(round(start_prespons,0))\n",
        "                end_prespons = max(int(round(start_spons-1)),0)\n",
        "                start_postspons = int(round(end_spons+1))\n",
        "                end_postspons = end_spons+sby2\n",
        "                end_postspons = int(round(end_postspons,0))\n",
        "\n",
        "                sub1 = end_prespons - start_prespons\n",
        "                sub2 = end_postspons - start_postspons\n",
        "                vid_id1 = str(vid_id)\n",
        "                if sub1 < sby2:\n",
        "                 end_postspons = end_postspons + sby2 - sub1\n",
        "\n",
        "                if  sub2 < sby2:\n",
        "                  start_prespons = start_prespons - sby2 + sub2\n",
        "                                                    #--------------------------------------------------------------------CHANGE FOLDER SAVE LOCATION FOR ALL 3 -------------------------------------------------------------------here-------------------------------\n",
        "                str_command1 = f''' ! ffmpeg -ss {start_prespons} -to {end_prespons} -i $(yt-dlp -f worstvideo --get-url \"https://www.youtube.com/watch?v={vid_id1}\") -vf fps=1 /content/drive/MyDrive/CNN_Dataset_Dump/shreyas/slot_1/non_sponsor/{vid_id1}_Prenonspons%d.jpg > /dev/null 2>&1 '''\n",
        "                os.system(str_command1)\n",
        "                str_command2 = f''' ! ffmpeg -ss {start_spons} -to {end_spons} -i $(yt-dlp -f worstvideo --get-url \"https://www.youtube.com/watch?v={vid_id1}\") -vf fps=1 /content/drive/MyDrive/CNN_Dataset_Dump/shreyas/slot_1/sponsor/{vid_id1}_Spons%d.jpg > /dev/null 2>&1 '''\n",
        "                os.system(str_command2)\n",
        "                str_command3 = f''' ! ffmpeg -ss {start_postspons} -to {end_postspons} -i $(yt-dlp -f worstvideo --get-url \"https://www.youtube.com/watch?v={vid_id1}\") -vf fps=1 /content/drive/MyDrive/CNN_Dataset_Dump/shreyas/slot_1/non_sponsor/{vid_id1}_Postnonspons%d.jpg > /dev/null 2>&1 '''\n",
        "                os.system(str_command3)\n",
        "                                                    #-------------------------------------------------------------------CHANGE FOLDER SAVE LOCATION FOR ALL 3 --------------------------------------------------------------------here------------------------------\n",
        "            yield 'gold'\n",
        "\n",
        "          else:\n",
        "              yield 'silver'\n",
        "\n",
        "        else:\n",
        "           yield 'silver'\n",
        "\n",
        "\n",
        "start1 = time.time()\n",
        "def process_video(vid_id):\n",
        "    \"\"\"\n",
        "    Downloads the captions for the video to the output\n",
        "    path save the job state to disk.\n",
        "    \"\"\"\n",
        "    global gold\n",
        "    global silver\n",
        "    global processed\n",
        "\n",
        "    try:\n",
        "        status = vid_process(vid_id)\n",
        "        status = next(status)\n",
        "        print(status)\n",
        "        if status == 'gold':\n",
        "            gold += 1\n",
        "        if status == 'silver':\n",
        "            silver += 1\n",
        "        with lock:\n",
        "            results.loc[len(results.index)] = [vid_id, status]\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return\n",
        "    processed += 1\n",
        "    elapsed = time.time() - start1\n",
        "    print()\n",
        "    print(f\"\"\"{processed} videos processed at a rate of {(elapsed / processed):.2f}s per video\n",
        "        {(gold / processed * 100):.2f}% of videos have Frames, {(silver / processed * 100):.2f}% of videos don't have Frames\n",
        "        {((processed + processed_last_session) / len(video_ids) * 100):.2f}% done\"\"\")\n",
        "\n",
        "sponsor_times = pd.read_csv(INPUT_FILENAME, sep=',', index_col=False)\n",
        "video_ids = sponsor_times['videoID'].unique()\n",
        "np.random.shuffle(video_ids)\n",
        "video_ids = list(video_ids)\n",
        "\n",
        "print()\n",
        "print(f'Found {len(video_ids)} videos')\n",
        "\n",
        "if processed_last_session > 0:\n",
        "    print(f' -->{processed_last_session} videos already processed.');\n",
        "    video_ids = set(video_ids).difference(set(results['videoID']))                 #2nd try error\n",
        "\n",
        "print(f' -->{len(video_ids)} videos remaining')\n",
        "\n",
        "#p = multiprocessing.dummy.Pool(processes=32)\n",
        "\n",
        "start = time.time()\n",
        "def _print_progress():\n",
        "    while True:\n",
        "        if processed == 0:\n",
        "            print('.', end='')\n",
        "            time.sleep(1)\n",
        "            continue\n",
        "        #--cut\n",
        "        with lock:\n",
        "            results.to_csv(STATUS_FILENAME + '~', index=False)\n",
        "            os.replace(STATUS_FILENAME + '~', STATUS_FILENAME)\n",
        "        time.sleep(5)\n",
        "\n",
        "timer = threading.Thread(target=_print_progress)\n",
        "timer.start()\n",
        "\n",
        "\n",
        "\n",
        "video_ids = list(video_ids)\n",
        "for i in range(20000): #no of video ID's to iterate in one sitting.\n",
        "  process_video(video_ids[i])"
      ],
      "metadata": {
        "id": "PmQ6ttiVyzLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8042d32-ab53-46c1-ebcc-ed7fb7db8ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuing...\n",
            "\n",
            "Found 754708 videos\n",
            " -->2 videos already processed.\n",
            " -->754707 videos remaining\n",
            ".first if\n",
            "got seg times 1\n",
            "got secfor\n",
            "start_spons is 0\n",
            "end_spons is 61\n",
            ".................................................gold\n",
            "\n",
            "1 videos processed at a rate of 10.12s per video\n",
            "        100.00% of videos have Intrest Graph, 0.00% of videos don't have Intrest Graph\n",
            "        0.00% done\n"
          ]
        }
      ],
      "id": "PmQ6ttiVyzLf"
    },
    {
      "cell_type": "code",
      "source": [
        "#Processed around 60k videos. Frames were captured at 1fps. For a vid, amt of frames captured were equal for both classes.\n",
        "#Eg: len sponsor seg = 20 secs then amt frames captured for both sponsor and non_sponsor is 20 each. Total = 20+20 frames: sponsor+non sponsor[pre+post]\n",
        "#___---pre-non-spons(10secs)--- ~spons (20secs)~ ---post-non-spons(10secs)---___________"
      ],
      "metadata": {
        "id": "ddQiPnjHw-Kk"
      },
      "id": "ddQiPnjHw-Kk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8652383",
      "metadata": {
        "id": "a8652383",
        "outputId": "00c757c4-26cb-4a1e-e2fb-c1713a523673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30G\tFINAL_CNN/Train/sponsor\n",
            "29G\tFINAL_CNN/Train/non_sponsor\n",
            "58G\tFINAL_CNN/Train\n",
            "3.7G\tFINAL_CNN/Valid/sponsor\n",
            "3.7G\tFINAL_CNN/Valid/non_sponsor\n",
            "7.3G\tFINAL_CNN/Valid\n",
            "3.9G\tFINAL_CNN/Test/sponsor\n",
            "3.9G\tFINAL_CNN/Test/non_sponsor\n",
            "7.7G\tFINAL_CNN/Test\n",
            "73G\tFINAL_CNN\n"
          ]
        }
      ],
      "source": [
        "#!du -h FINAL_CNN          #verifying dataset size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed68bb96",
      "metadata": {
        "id": "ed68bb96",
        "papermill": {
          "duration": 1.561202,
          "end_time": "2022-01-04T16:42:52.641592",
          "exception": false,
          "start_time": "2022-01-04T16:42:51.080390",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T # for simplifying the transforms\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random_split\n",
        "from torchvision import models\n",
        "from PIL import Image, ImageFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99570fd9",
      "metadata": {
        "id": "99570fd9",
        "papermill": {
          "duration": 10.452157,
          "end_time": "2022-01-04T16:43:03.111703",
          "exception": false,
          "start_time": "2022-01-04T16:42:52.659546",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Now, we import timm, torchvision image models\n",
        "#!pip install timm\n",
        "import timm\n",
        "from timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e185ef33",
      "metadata": {
        "id": "e185ef33",
        "papermill": {
          "duration": 0.027079,
          "end_time": "2022-01-04T16:43:03.159932",
          "exception": false,
          "start_time": "2022-01-04T16:43:03.132853",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# remove warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c64d65",
      "metadata": {
        "id": "a4c64d65",
        "papermill": {
          "duration": 0.028813,
          "end_time": "2022-01-04T16:43:03.208860",
          "exception": false,
          "start_time": "2022-01-04T16:43:03.180047",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a09ca063",
      "metadata": {
        "id": "a09ca063",
        "papermill": {
          "duration": 0.026094,
          "end_time": "2022-01-04T16:43:03.255513",
          "exception": false,
          "start_time": "2022-01-04T16:43:03.229419",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd0fdc0",
      "metadata": {
        "id": "ccd0fdc0",
        "papermill": {
          "duration": 0.026155,
          "end_time": "2022-01-04T16:43:03.301828",
          "exception": false,
          "start_time": "2022-01-04T16:43:03.275673",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_classes(data_dir):\n",
        "    all_data = datasets.ImageFolder(data_dir)\n",
        "    return all_data.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a85e72c6",
      "metadata": {
        "id": "a85e72c6"
      },
      "outputs": [],
      "source": [
        "def check_Image(path):\n",
        "  try:\n",
        "    im = Image.open(path)\n",
        "    return True\n",
        "  except:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f0af54",
      "metadata": {
        "id": "a7f0af54",
        "papermill": {
          "duration": 0.032846,
          "end_time": "2022-01-04T16:43:03.355342",
          "exception": false,
          "start_time": "2022-01-04T16:43:03.322496",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_data_loaders(data_dir, batch_size, train = False):\n",
        "    if train:\n",
        "        #train\n",
        "        transform = T.Compose([\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.RandomVerticalFlip(),\n",
        "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.25),\n",
        "            T.Resize(256),\n",
        "            T.CenterCrop(224),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n",
        "            T.RandomErasing(p=0.3, value='random') #0.2\n",
        "        ])\n",
        "        train_data = datasets.ImageFolder(os.path.join(data_dir, \"Train/\"), transform = transform, is_valid_file=check_Image)\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=5)\n",
        "        print('trainload')\n",
        "        return train_loader, len(train_data)\n",
        "    else:\n",
        "        # val/test\n",
        "        transform = T.Compose([ # We dont need augmentation for test transforms\n",
        "            T.Resize(256),\n",
        "            T.CenterCrop(224),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n",
        "        ])\n",
        "        val_data = datasets.ImageFolder(os.path.join(data_dir, \"Valid/\"), transform=transform, is_valid_file=check_Image)\n",
        "        print('vald')\n",
        "        test_data = datasets.ImageFolder(os.path.join(data_dir, \"Test/\"), transform=transform, is_valid_file=check_Image)\n",
        "        print('testd')\n",
        "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=5)\n",
        "        print('valload')\n",
        "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=5)\n",
        "        print('testload')\n",
        "        return val_loader, test_loader, len(val_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1a5a08",
      "metadata": {
        "id": "7e1a5a08"
      },
      "outputs": [],
      "source": [
        "current_folder = globals()['_dh'][0]\n",
        "\n",
        "# Calculating path to the input data\n",
        "data_location = os.path.join(current_folder,'FINAL_CNN/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac34e17",
      "metadata": {
        "id": "6ac34e17"
      },
      "outputs": [],
      "source": [
        "current_folder = globals()['_dh'][0]\n",
        "\n",
        "# Calculating path to the input data\n",
        "data_location2 = os.path.join(current_folder,'FINAL_CNN/Train/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca93367",
      "metadata": {
        "id": "5ca93367",
        "papermill": {
          "duration": 0.026316,
          "end_time": "2022-01-04T16:43:03.402233",
          "exception": false,
          "start_time": "2022-01-04T16:43:03.375917",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset_path = data_location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c26c4ad6",
      "metadata": {
        "id": "c26c4ad6",
        "outputId": "fd6bfbc7-bcf5-498f-b6ad-13707a922b89",
        "papermill": {
          "duration": 2.944136,
          "end_time": "2022-01-04T16:43:06.366523",
          "exception": false,
          "start_time": "2022-01-04T16:43:03.422387",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainload\n",
            "vald\n",
            "testd\n",
            "valload\n",
            "testload\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "(train_loader, train_data_len) = get_data_loaders(dataset_path, 128, train=True) #495 512\n",
        "(val_loader, test_loader, valid_data_len, test_data_len) = get_data_loaders(dataset_path, 64, train=False) #40\n",
        "print('1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c6af89f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c6af89f",
        "outputId": "c3b3f5a8-2461-4897-dcfc-6f6a82b67ac9",
        "papermill": {
          "duration": 0.101169,
          "end_time": "2022-01-04T16:43:06.488811",
          "exception": false,
          "start_time": "2022-01-04T16:43:06.387642",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['non_sponsor', 'sponsor'] = 2\n"
          ]
        }
      ],
      "source": [
        "classes = get_classes(data_location2)\n",
        "print(classes,'=', len(classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9ad9be",
      "metadata": {
        "id": "3a9ad9be",
        "papermill": {
          "duration": 0.027854,
          "end_time": "2022-01-04T16:43:06.537174",
          "exception": false,
          "start_time": "2022-01-04T16:43:06.509320",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\": val_loader\n",
        "}\n",
        "dataset_sizes = {\n",
        "    \"train\": train_data_len,\n",
        "    \"val\": valid_data_len\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28942e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b28942e9",
        "outputId": "b449e5b7-ae07-41ee-b9f5-d22513d69212",
        "papermill": {
          "duration": 0.029705,
          "end_time": "2022-01-04T16:43:06.587591",
          "exception": false,
          "start_time": "2022-01-04T16:43:06.557886",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44856 11543 12011\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader), len(val_loader), len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7808f199",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7808f199",
        "outputId": "56cff5ec-43ca-45cb-a25c-4bf56051496e",
        "papermill": {
          "duration": 0.028905,
          "end_time": "2022-01-04T16:43:06.638800",
          "exception": false,
          "start_time": "2022-01-04T16:43:06.609895",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5741450 738752 768704\n"
          ]
        }
      ],
      "source": [
        "print(train_data_len, valid_data_len, test_data_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf4b6250",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf4b6250",
        "outputId": "c3adf269-a098-423b-cd66-1b33122465ff",
        "papermill": {
          "duration": 0.076145,
          "end_time": "2022-01-04T16:43:06.735827",
          "exception": false,
          "start_time": "2022-01-04T16:43:06.659682",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# now, for the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5964e4dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5964e4dc",
        "outputId": "71c022fd-d716-4622-8755-c2fc22221da4",
        "papermill": {
          "duration": 2.209363,
          "end_time": "2022-01-04T16:43:08.966463",
          "exception": false,
          "start_time": "2022-01-04T16:43:06.757100",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/fsuser/.cache/torch/hub/facebookresearch_deit_main\n"
          ]
        }
      ],
      "source": [
        "model_parent = torch.hub.load('facebookresearch/deit:main', 'deit_small_patch16_224', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85aa92b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c85aa92b",
        "outputId": "191d76bb-c770-4cda-baf5-73e944020031",
        "papermill": {
          "duration": 2.839727,
          "end_time": "2022-01-04T16:43:11.828987",
          "exception": false,
          "start_time": "2022-01-04T16:43:08.989260",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=384, out_features=2048, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            "  (3): Linear(in_features=2048, out_features=1524, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=1524, out_features=1024, bias=True)\n",
            "  (6): Dropout(p=0.16, inplace=False)\n",
            "  (7): Linear(in_features=1024, out_features=800, bias=True)\n",
            "  (8): Linear(in_features=800, out_features=512, bias=True)\n",
            "  (9): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "for param in model_parent.parameters(): #IMPPPPPPPPPPPP\n",
        "    param.requires_grad = False\n",
        "\n",
        "n_inputs = model_parent.head.in_features\n",
        "model_parent.head = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 2048),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(2048, 1524),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1524, 1024),\n",
        "    nn.Dropout(0.16),\n",
        "    nn.Linear(1024, 800),\n",
        "    nn.Linear(800, 512),\n",
        "    nn.Linear(512, 2)  #2 classes ----/\n",
        ")\n",
        "model2= model_parent.to(device)\n",
        "print(model2.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e83a3a9c",
      "metadata": {
        "id": "e83a3a9c",
        "outputId": "dd62be2b-4227-4112-ae32-4578afa17903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=384, out_features=2048, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            "  (3): Linear(in_features=2048, out_features=1524, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=1524, out_features=1024, bias=True)\n",
            "  (6): Dropout(p=0.16, inplace=False)\n",
            "  (7): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=800, bias=True)\n",
            "    (1): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (8): Linear(in_features=800, out_features=512, bias=True)\n",
            "  (9): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define the dropout layer to be added\n",
        "dropout_layer = nn.Dropout(0.3)\n",
        "\n",
        "# Get the reference to the existing layer\n",
        "old_conv = model2.head[7]\n",
        "\n",
        "# Create a new sequential module with the dropout layer followed by the existing layer\n",
        "new_conv = nn.Sequential(\n",
        "    old_conv, dropout_layer\n",
        ")\n",
        "\n",
        "# Update the model with the new sequential module\n",
        "model2.head[7] = new_conv\n",
        "\n",
        "print(model2.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76674417",
      "metadata": {
        "id": "76674417",
        "outputId": "5cc38bc8-c7b7-4cb1-af2d-5f1850cdcbe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=384, out_features=2048, bias=True)\n",
            "  (1): Sequential(\n",
            "    (0): Identity()\n",
            "  )\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            "  (3): Linear(in_features=2048, out_features=1524, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=1524, out_features=1024, bias=True)\n",
            "  (6): Dropout(p=0.16, inplace=False)\n",
            "  (7): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=800, bias=True)\n",
            "    (1): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (8): Linear(in_features=800, out_features=512, bias=True)\n",
            "  (9): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define the dropout layer to be added\n",
        "new_layer = nn.Identity()\n",
        "\n",
        "# Get the reference to the existing layer\n",
        "old_conv = model2.head[1]\n",
        "\n",
        "# Create a new sequential module with the dropout layer followed by the existing layer\n",
        "new_conv = nn.Sequential(new_layer)\n",
        "\n",
        "# Update the model with the new sequential module\n",
        "model2.head[1] = new_conv\n",
        "\n",
        "print(model2.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93f8a34f",
      "metadata": {
        "id": "93f8a34f",
        "outputId": "10599d3a-d499-47a0-acf1-b32ece65b4e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=384, out_features=2048, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.25, inplace=False)\n",
            "  (3): Linear(in_features=2048, out_features=1524, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=1524, out_features=1024, bias=True)\n",
            "  (6): Dropout(p=0.15, inplace=False)\n",
            "  (7): Linear(in_features=1024, out_features=800, bias=True)\n",
            "  (8): Dropout(p=0.2, inplace=False)\n",
            "  (9): Linear(in_features=800, out_features=512, bias=True)\n",
            "  (10): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "for param in model_parent.parameters(): #freeze model\n",
        "    param.requires_grad = False\n",
        "\n",
        "n_inputs = model_parent.head.in_features\n",
        "model_parent.head = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 2048),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.25),\n",
        "    nn.Linear(2048, 1524),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1524, 1024),\n",
        "    nn.Dropout(0.15),\n",
        "    nn.Linear(1024, 800),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(800, 512),\n",
        "    nn.Linear(512, 2)  #2 classes ----/\n",
        ")\n",
        "model2= model_parent.to(device)\n",
        "print(model2.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8d2ed20",
      "metadata": {
        "id": "f8d2ed20",
        "outputId": "a4315c01-ae44-48fc-c1d3-19a9c7faef63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=384, out_features=2048, bias=True)\n",
            "  (1): Sequential(\n",
            "    (0): Identity()\n",
            "  )\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            "  (3): Linear(in_features=2048, out_features=1524, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=1524, out_features=1024, bias=True)\n",
            "  (6): Dropout(p=0.16, inplace=False)\n",
            "  (7): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=800, bias=True)\n",
            "    (1): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (8): Linear(in_features=800, out_features=512, bias=True)\n",
            "  (9): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model2.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e37f5cf3",
      "metadata": {
        "id": "e37f5cf3",
        "papermill": {
          "duration": 0.029644,
          "end_time": "2022-01-04T16:43:11.937571",
          "exception": false,
          "start_time": "2022-01-04T16:43:11.907927",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# load the saved information back\n",
        "checkpoint = torch.load('DeiT_dense_256batch_epoch_3.pt')\n",
        "optimizer = torch.optim.AdamW(model2.head.parameters(), lr=0.001, weight_decay=0.1)\n",
        "model2.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962314f5",
      "metadata": {
        "id": "962314f5",
        "outputId": "d03d3ff3-c9e9-4a9c-8c2d-fe8cef93c683"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AdamW (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    lr: 0.0001\n",
              "    maximize: False\n",
              "    weight_decay: 0.1\n",
              ")"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion = LabelSmoothingCrossEntropy()\n",
        "criterion = criterion.to(device)\n",
        "optimizer.param_groups[0]['weight_decay'] = 0.1\n",
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a99274c",
      "metadata": {
        "id": "3a99274c"
      },
      "outputs": [],
      "source": [
        "exp_lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "    factor=0.1, patience=1, threshold=0.0001, threshold_mode='abs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a90b32d",
      "metadata": {
        "id": "0a90b32d",
        "papermill": {
          "duration": 0.039385,
          "end_time": "2022-01-04T16:43:12.000940",
          "exception": false,
          "start_time": "2022-01-04T16:43:11.961555",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "ite = 1\n",
        "def train_model(model2, criterion, optimizer, scheduler, num_epochs=3):  #5epochs;\n",
        "    global ite\n",
        "\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model2.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print(\"-\"*10)\n",
        "        model_save_name = 'DeiT_dense_128batch_epoch_' + str(ite) + '.pt'\n",
        "\n",
        "        for phase in ['train', 'val']: # We do training and validation phase per epoch\n",
        "            if phase == 'train':\n",
        "                model2.train() # model to training mode\n",
        "            else:\n",
        "                model2.eval() # model to evaluate\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n",
        "                    outputs = model2(inputs)\n",
        "                    _, preds = torch.max(outputs, 1) # used for accuracy\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step(loss) # step at end of epoch\n",
        "                torch.save({'model_state_dict': model2.state_dict(),'optimizer_state_dict': optimizer.state_dict()\n",
        "                           }, model_save_name)\n",
        "                ite = ite+1\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model2.state_dict()) # keep the best validation accuracy model\n",
        "\n",
        "        print()\n",
        "    time_elapsed = time.time() - since # slight error\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
        "\n",
        "    model2.load_state_dict(best_model_wts)\n",
        "    return model2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113bcd9f",
      "metadata": {
        "id": "113bcd9f",
        "outputId": "9950e994-5f8d-4f75-9f1d-563522e946aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/6\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 11214/11214 [1:22:12<00:00,  2.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.5325 Acc: 0.7582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 23086/23086 [13:08<00:00, 29.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.6118 Acc: 0.6787\n",
            "\n",
            "Epoch 1/6\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 11214/11214 [1:21:40<00:00,  2.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.5450 Acc: 0.7464\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 23086/23086 [13:09<00:00, 29.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.6142 Acc: 0.6784\n",
            "\n",
            "Epoch 2/6\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 11214/11214 [1:20:57<00:00,  2.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.5499 Acc: 0.7417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 23086/23086 [13:07<00:00, 29.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.6123 Acc: 0.6775\n",
            "\n",
            "Epoch 3/6\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 11214/11214 [1:20:56<00:00,  2.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.5134 Acc: 0.7737\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 23086/23086 [13:11<00:00, 29.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.6098 Acc: 0.6864\n",
            "\n",
            "Epoch 4/6\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|█████████████████████████████████████████████████████████████████           | 9602/11214 [1:09:12<11:22,  2.36it/s]IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "100%|███████████████████████████████████████████████████████████████████████████| 11214/11214 [1:20:53<00:00,  2.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.4949 Acc: 0.7897\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|███████████████████████████████▉                                              | 9471/23086 [05:22<07:34, 29.97it/s]"
          ]
        }
      ],
      "source": [
        "model_ft5 = train_model(model2, criterion, optimizer, exp_lr_scheduler2) #Epoch 1-7 and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455bf3d7",
      "metadata": {
        "scrolled": true,
        "id": "455bf3d7",
        "outputId": "3ad3c296-a49d-45d1-a0f1-6d6574b87a68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/4\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 11214/11214 [1:24:10<00:00,  2.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.5393 Acc: 0.7516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 23086/23086 [13:05<00:00, 29.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.6234 Acc: 0.6764\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 11214/11214 [1:24:42<00:00,  2.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.5415 Acc: 0.7497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 23086/23086 [13:07<00:00, 29.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.6233 Acc: 0.6783\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 11214/11214 [1:24:16<00:00,  2.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.5407 Acc: 0.7506\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 23086/23086 [13:05<00:00, 29.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.6150 Acc: 0.6744\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 11214/11214 [1:24:10<00:00,  2.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.5398 Acc: 0.7514\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 23086/23086 [13:06<00:00, 29.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.6208 Acc: 0.6766\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 11214/11214 [1:24:18<00:00,  2.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.4938 Acc: 0.7911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 23086/23086 [13:02<00:00, 29.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.6190 Acc: 0.6848\n",
            "\n",
            "Training complete in 487m 9s\n",
            "Best Val Acc: 0.6848\n"
          ]
        }
      ],
      "source": [
        "model_ft6 = train_model(model2, criterion, optimizer, exp_lr_scheduler2) # Epoch 8-12. We choose the final epoch save file ~ 12 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e28eb9",
      "metadata": {
        "id": "04e28eb9"
      },
      "outputs": [],
      "source": [
        "torch.save({'model_state_dict': model_ft6.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                       }, 'Deit_Dense_AdamwFINAL_epoch12.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32bace68",
      "metadata": {
        "id": "32bace68",
        "papermill": {
          "duration": 0.154874,
          "end_time": "2022-01-04T16:46:24.778032",
          "exception": false,
          "start_time": "2022-01-04T16:46:24.623158",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5492d5d",
      "metadata": {
        "id": "c5492d5d",
        "papermill": {
          "duration": 0.165342,
          "end_time": "2022-01-04T16:46:25.097060",
          "exception": false,
          "start_time": "2022-01-04T16:46:24.931718",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST of Deit_Dense_AdamwFINAL_epoch12.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b03623e6",
      "metadata": {
        "id": "b03623e6",
        "papermill": {
          "duration": 1.400232,
          "end_time": "2022-01-04T16:46:26.651083",
          "exception": false,
          "start_time": "2022-01-04T16:46:25.250851",
          "status": "completed"
        },
        "tags": [],
        "outputId": "3ad07785-bf43-4c16-c6eb-b9a94cbeff30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 24022/24022 [14:21<00:00, 27.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.0000\n",
            "Test Accuracy of non_sponsor: 75% (288188/380178)\n",
            "Test Accuracy of sponsor: 66% (256670/388526)\n",
            "Test Accuracy of 70% (544858/768704)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_loss = 0.0\n",
        "class_correct = list(0 for i in range(len(classes)))\n",
        "class_total = list(0 for i in range(len(classes)))\n",
        "model_ft6.eval()\n",
        "\n",
        "for data, target in tqdm(test_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    with torch.no_grad(): # turn off autograd for faster testing\n",
        "        output = model_ft6(data)\n",
        "        loss = criterion(output, target)\n",
        "    test_loss = loss.item() * data.size(0)\n",
        "    _, pred = torch.max(output, 1)\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    if len(target) == 32:\n",
        "        for i in range(32):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "test_loss = test_loss / test_data_len\n",
        "print('Test Loss: {:.4f}'.format(test_loss))\n",
        "for i in range(len(classes)):\n",
        "    if class_total[i] > 0:\n",
        "        print(\"Test Accuracy of %5s: %2d%% (%2d/%2d)\" % (\n",
        "            classes[i], 100*class_correct[i]/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n",
        "        ))\n",
        "    else:\n",
        "        print(\"Test accuracy of %5s: NA\" % (classes[i]))\n",
        "print(\"Test Accuracy of %2d%% (%2d/%2d)\" % (\n",
        "            100*np.sum(class_correct)/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n",
        "        ))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 228.236076,
      "end_time": "2022-01-04T16:46:31.292102",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-01-04T16:42:43.056026",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}